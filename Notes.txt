
NOTES WRITTEN BY MUHAMMAD SUHAIB
LINKEDIN : linkedin.com/in/msuhaib-03/fsjavadev/
GITHUB: github.com/msuhaib-03
Gmail: muhammadsuhaib2805@gmail.com



------------------------------------ LECTURE 1 -------------------------------------
Spring Boots is framework for bulding application in Java --> it basically provides tools
Easy to create stand-alone, production-grade Spring Based Applications 

Spring framework reduce boilerplate code, reduce effort to setup and config Spring Application.
Auto-config & StandAlone App, No need for Tomcat server, No need to config any server or its config !!

Annotation is a hint and @SpringBootApplication replaces need for setting Spring Application manually.
It comes with a lot of automatic configuration
Spring	Application.run() to start the application

Bean can be reused. --> Bean is an object
ApplicationContext is asked to provide bean. ---> It creates and provides you bean

------------------------------------ LECTURE 2 -------------------------------------

Download & Installation of IntelliJ
Adoptium Eclipse

------------------------------------ LECTURE 3 -------------------------------------

Java8 & Java11 are the most stable
We will do version upgrade as well later on which is work of developer.

------------------------------------ LECTURE 4 -------------------------------------

Spring Boot Project ( Journal App )
jar is run just like that where as war has to be deployed on external server
Spring Initializer
Maven helps to convert to jar or war
Add Dependencies --> Spring Web for APIs & uses Apache Tomcat Server
Added RestController to for  @GetMapping for context on localhost:8080


------------------------------------ LECTURE 5 -------------------------------------

Download & Install maven

------------------------------------ LECTURE 6 -------------------------------------

Maven creates build --> build simplify, automate, dependency manage
All jars are available and downloaded & used

Maven repository has all the dependencies you need for your project, it will download the jar in external libraries ---> Dependency

validate, compile, test, package, verify, install, deploy ---> Build lifecycle of Maven

Make sure to be in the same folder as project where all the files and folders are lying
mvn validate, mvn compile, mvn test, mvn package( no need to write all the commands again and again, just package is enough for all work) will make jar,
mvn install ( this creates a path where the jar will be stored and can be used again in future ), mvn clean ( removes all that was present or created using mvn )

java -jar <file name of jar in target folder>  --> runs project

pom means build tool is maven

------------------------------------ LECTURE 7 -------------------------------------

.idea --> intellij related config
.mvn --> maven wrapper
src -> main ( functionality )
src -> test ( testing )
pom.xml --> all info related to project, external libraries

plugin, build --> packaging code to jar or war 

parent fetch required dependencies 

.jar.original only has compiled code
.jar has the plugins, dependencies, compiled code, self-contained server ( self-contained jar) --> fat jar

mvn package also does the repackaging, repackages the original jar and jar

------------------------------------ LECTURE 8 -------------------------------------

We used to manually create object i.e: car c = new car();
Externalize the object creation --> Inversion of Control  ( Spring provides IOC Container ) 
All the classes in project are stored in IOC Container, no need to create object, it wil be done by IOC.

Control is inverted, all the objects are present in a box called IOC.
ApplicationContext is a way to apply IOC. 

IOC scans the classes but adds only those which contain:
@Component, --> if this annotation is present at any class, IOC will accept and save this class in it.
Class,interface,method,field. ( @ tells some info about those )

@Component makes Beans
Bean -> object
The object in IOC container is called Bean.

@SpringBootApplication -> it runs the entire project, it is entry point and main method ( it is only 1 and it is only going to be on main class )
All the classes, beans must be made in base class / package, oonly then it could be scanned by SpringBootApplication
It contains 3 Annotations:
@Configuration --> Bean creation of that class
@EnableAutoConfiguration --> There's no need to setup things manually, spring boot does it all by itself i.e mongo db setup, server setup
@ComponentScan --> IOC Container / Scanning / Scan Beans

@Autowired, --> dependency injection
if a class needs to be used in another class, the one we want to use, write the @Autowired annotation on it, so its called dependency injection.
There's no need to make object, it gives direct access to that class's objects, methods, interface.
By using it, any calss in future can use this single instance

@Bean, --> this can be used to crate bean in a class but it is only used on a function


------------------------------------ LECTURE 9 -------------------------------------

New SpringBoot Project creation.

Spring Initializer:
Lang -> Java
Project -> Maven
SpringBoot -> ( select older version )
Group: Name of company in reverse fashion
Artifact: Name of project ( must begin with lowercase )
Name: ( Any name -- must be human-readable )
Description: ( Any description )
Packaging: Jar
Java : ( Select older version, either one installed in your pc or a little above your version )
Dependencies: Spring Web

Representative State Transfer & Application Programming Interface
REST API --> How to access something on server , therefore ( GET + 172.17.18.19:8080/netflix/plans ) 
URL+Endpoint( 122.76.0.45:8080 + /plans/pricing )

URL + HTTP Verb
GET, PUT, POST, DELETE ( See, Modify, Create, Remove )

@RestController at class will be a Bean --> it is used as a component but providing some additional functionality as well
End-points are written as method

Postman is used to check API
@RequestMapping adds mapping to entire class and then finds the sub mappings in that class
Methods inside a controller class should be public so that they can be accessible and invoked by the spring framework or external http requests.
Selecting "raw" and "JSON" in body of POST request in Postman indicates that request body will contain data in JSON format, allowing the server to parse and process
the incoming data accurately.

@RequestBody in a parameter of function is like: hey spring, take the data from the request and turn it into java object that I can use in my code
@PathVariable --> have to give the path to the endpoint for api hit

? --> this is request parameter
/ --> this is path variable

:)

------------------------------------ LECTURE 10 -------------------------------------

MongoDB installation on Windows.

------------------------------------ LECTURE 11 -------------------------------------

MongoDB is database and it has collection not table.
Collection has fields(columns) and rows(documents).

Run cmd as an administrator and type mongosh.
show dbs
use <name of db to create>
show collections  --> no tables created yet.
db.createCollection <name of table>
db.students.insertOne({"name":"Sobi", "age":22})
db.students.insertOne({"name":"Rabiya", "age":27})

What it is actually doing is that it goes to current selected database,create a table if not like here <students> and then write the rows as JSON.

db.students.find() ---> show the data in the table
db.students.find().pretty() ---> show in a proper manner of documents(rows)
db.students.find({name:"Rabiya"}) --> It finds exact field through JSON
db.students.deleteOne({name:"Suhaib"})


------------------------------------ LECTURE 12 -------------------------------------

ORM --> ( Object Relational Model ) : technique used to map java objects to database tables
Allows devs to work with database using OOP, making it easier to interact with relational database.

Consider a Java class called Users and db called users. ORM maps the fields in User class to columns in the users table, making easier to insert,upd,del,retrieve.

JPA ---> Java Persistence API 
Provide set rules to achieve ORM
It acts as a bridge between OOP & relational database.

Persistence Provider / ORM Tools ---> To use JPA, you need persistence provider.
JPA persistence providers include Hibernate, EclipseLink and OpenJPA.  ( You will use JPA but by which means you want to use JPA )
We use these to interact with our databases

Spring Data JPA --> It simplifies working with JPA. You will still need a JPA provider.

MongoDB is a NoSQL database that uses a different data model, typically based on collections of documents, which are schema-less or have flexible schemas.
JPA is not used with MongoDB.
As it is a NoSQL database so SpringData JPA serves as the persistence provider for MongoDB.

QueryMethodDSL & CriteriaAPI are 2 diff ways to interact with database:
SpringDataJPA for Relational database.
SpringData MongoDB for MongoDB Database.

QueryMethodDSL: create queries based on method naming conventions
CriteriaAPI : dynamic & programmatic approach for complex and custom queries


------------------------------------ LECTURE 13 -------------------------------------

Add springdb mongodb dependency in pom.xml

Goto application.properties in resources folder and provide this info:
spring.data.mongodb.host=localhost
spring.data.mongodb.port=27017

This local host and port can vary if you work at office or somewhere.

Now create a package called service and class within it which will hold all the business logic here.
We will also create a an interface of repository within a repository package

Controller calls the service, service calls repo.
controller ---> service --> repo(interface)

Repo runs query from database. The database that you wanna play with, extends that in repo interface.
extends MongoRepository
MongoRepository is an interface provided by default SpringDataMongoDB contains the code for table(collection) when ctrl+right-click.
It will contain 2 parameters.

@Document to be written on our pojo class to make it equals to row for db. ( The pojo class will be equal to 1 row containing all fields in it)
@Id makes an initialization unique/primary key.

In the v2 controller, we @AutoWired the JourmalEntryService so no need to rewrite it for object.

Now run the program and hit the postman with a text body with json to check if POST verb is working.
Then Open the mongosh and hit show dbs, then use journaldb, show collections --> ( journalEntry is shown )
db.journalEntry.find() ---> just show the data we hit from POST in Postman
If you don't have or enter id, mongoDB itself generates a variable of ObjectID with a unique id for each document.
You may delete the entries from table now in mongosh and goto JournalEntry class and convert the String Id to ObjectId.
Add LocalDateTime as well.
Optional is data type where there could be data and there could not be.

Create all the services in journalEntryService --> All what you need, Save, Update, List, Create
JournalEntry has all the ---> Variables declared with getters and setters.
JournalEntryRepo extends MongoRepository and nothing else in code.
JournalEntryController is where you hit the APIs and use @RestController with all the HTTP Verbs for the services you need.

In the JournalEntry, where you wrote @Document(collection="journal_entries")
you have to use the same document is mongosh for accessing it.
Like: db.journal_entries.find()

ALHAMDULILAH!!!! Troubleshooted the problem in IntelliJ & Mongosh by self after analyzing the errors generated and help from GPT.
APIs are being hit now .. :)

------------------------------------ LECTURE 14 -------------------------------------
Request & Response
Response Entity + HTTP Code
3 digit numeric code returned by a server. These are used to convey information about the result or status of requested operation.
HTTP Codes
1xx --> Information ( request was received and understood & processed, rarely seen in practice )
2xx --> Successful ( request was successfully received, understood & processed by the server ) -> 200,201,204
3xx --> Redirection ( further action needed to complete the request. Used when client needs to take additional action to access the requested resource ) -> 301(Resource
has been permanently moved to a different URL), 302( Resource Temporarily moved to a different URL, Redirect field alsi given), 304(Cached version can be used)
4xx --> Error on client's part, authentication issues, 400(Bad Request), 401(Unauthorized), 403(Forbidden)
5xx --> Server Error ,500(Internal Server Error), 502(Bad Gateway). 503(Service Unavailable/temp overloading or maintenance)

Response Entity class is a part of Spring Framework and is commonly used in SpringBoot apps to customize HTTP response.
Provides methods for setting reponse status,header & body.
Make changes in the JournalEntryController and add the ResponseEntity with status codes. 



------------------------------------ LECTURE 15 -------------------------------------

Lombok is popular library in Java ecosystem, often used in SpringBoot apps. It aims to reduce boilerplate. Lombok achieves this by generating this code automatically
during compilation based on annotations you add to your Java class.

Goto Project Lombok, select install ( maven ) and add the dependency to pom.xml

In your entity vlass where you need to do the work reduction, add the annotations there like:
@Getter, @Setter, @Builder, &EqualsandHashCode, @ToString
Also install plugin of Lombok in intelliJ
But @Data annotation of Lombok is equals to all the above written annotations,it includes all of them.
Lombok generates bytecode for methods as speicifed by the annotations used in your code. This generated code is added to the compiled class files (.class).

Java cmopiler compiles your classes, including the generated code. This means that the generated methods become part of your compiled class files.
When you run your app the generated methods are available for use, just like any other method in your class.



------------------------------------ LECTURE 16 -------------------------------------

Creating a connection between JournalEntry & User
@DBRef to make a link of a database into another one, and it works as a foreign key. JournalEntries in our project is making a reference, link making between 2 collections.
Parent-child relationship is established here in User.java class
spring.data.mongodb.auto-index-creation=true  --> This has to be added to application.properties for username indexing
Make a UserController,UserRepo,UserService same as the journal classes but do work as following the service class
In UserRepo make a method of username.

Now to create a connection betwwen the User & Journal, we will start wokring on the JournalControllerV2. 
Do the Autowire UserService class in JournalEntryController and use @PathVariable to find userName.
All the journalEntriesof that particular user will appear.

UserName + journalEntry --> API 
userName comes from user and user has id,username,password and journal entries.
journalEntry has journalEntries.
Id is saved in journalEntries of journalEntry.
So this API makes enter journalEntry into journalEntries & it also saves the user's journalEntries's id from users.

Everything good till now!!! Get,Post,Put,Delete working and their connection of users and journal_entries are also working as expected.
Working on cascade mongodb as deleting an entry from journal_entries doesn't delete from the user, it is stil there automatically, therefore has to be done manually now.

Done the work  on JournalEntryService for deletion.
Create JournalEntry on postman:
localhost:8080/journal/<userName>   --> POST method
db.journal_entries.find()  --> entry created
db.users.find() --> entry also created here

Deletion on postman:
localhost:8080/journal/id/<userName>/<objectid from journal_entries>
db.journal_entries.find() --> entry is deleted
db.users.find() --> entry deleted from here too

Consistency will  come on users' next save but we want it currently delete, delete instantly. 

Now all the HTTP verbs are working and connection/relation made between users and journal_entries collection.

Problem here is that the transaction between the users and journal_entries has to be same all the time and at the same times. If any error occurs in saving journalEntry,
it is going in JournalEntry and Users collection as well, it should occur as one operation.

------------------------------------ LECTURE 17 -------------------------------------

@Transactional -->  will be used in case of transaction where if one succeeds, means all will succeed, but if one is wrong or discarded then all will be discarded.

@EnableTransactionManagement --> it is added on main SpringBootApplication.
What happens here is that it creates a container, which contains all the methods that have @Transactional annotaion used on it and it provides the area for springboot
application to search and make the application and transaction atomic( if one fails, all will fail, if one passes, all will pass )
Isolation is also achieved by using it. 

All of it is basically done by an Interface PlatformTransactionManager which is responsible for transactions, commit & rollback.
MongoTransactionManager is class which implements it. --> it is all wokring behind.
A @Bean of platformtransactionmanager is created in main app which has mongodatabasefactory as param & return instance of mongotransactionmanager.
MongoDatabaseFactory( an interace ) helps is building connection with the database --> used as parameter in method of PlatformTransactionManager.
And it is an implementation of SimpleMongoClientDatabaseFactory.

You can also create a package with class TransactionConfig for the above mentioned things rather than creating them in main SpringBootApplication but we will keep it.


------------------------------------ LECTURE 18 -------------------------------------

Create an account on MongoDB Atlas.
Do the personalization, and select the free plan.
Select AWS as Provider & Region as you like and then create Deployment after giving it a name.
(password), (username)

mongodb+srv://muhammadsuhaib2805:<db_password>@cluster0.9wu5ofl.mongodb.net/?retryWrites=true&w=majority&appName=Cluster0 ---> ( paste in springboot project )
This is obtained from the connect button & then drivers option.

Don't forget to hit add my current ip address to make it whitelist for use in Quickstart Security option.
Finish & Close.

Cluster is a formation from nodes means servers. Data is evenly distributed so if a server goes down, data can be fetched through another( It is called sharding ). 

Paste the uri in springbootapp in application.properties with:
spring.data.mongodb.uri=mongodb+srv://sobi7:<db_password>@cluster0.ar8adhd.mongodb.net/?retryWrites=true&w=majority&appName=Cluster0
This is hard-coded and not a good practice tho for now but we will externalize it later.

___________________________ SERIOUS PROBLEM OCCURED HERE!!!!! JAVA VERSION 8 IS NOT COMPATIBLE WITH CURRENT SPRINGBOOT VERSION THEREFORE IT IS NOT GOING TO RUN
ANYTHING ON MONGODB ATLAS. IT TOOK A LOT OF HOURS AND TROUBLESHOOTING TO KNOW WHAT WENT WRONG. DOWNLOAD JAVA 17 NOW, SET JDK, SDK___________________________


DAYSS AND DAYSSS PASSED!!!!! FINALLY TROUBLESHOOTED THE PROBLEM, IT WAS THE PROXY THAT WAS STOPPING US TO CONNECT, EVERYTHING WAS FINE, THE CONNECTION STRING, THE
SPRING BOOT PROJECT :)
TOOK APPROX 3 DAYS TO KNOW WHY IS IT HAPPENING AND DID ALL ON MY OWN, NO HELP FROM TUTORIALS OR GPT...


Connection String would look something like this:
mongodb+srv://sobi7:<db_password>@cluster0.ar8adhd.mongodb.net/?retryWrites=true&w=majority&appName=Cluster0

I recommend doing all the stuff on Studio3t and it can be connected to MongoDB Atlas

------------------------------------ LECTURE 19 -------------------------------------

SpringSecurity class download. Only applies to latest users of springboot with ver 3.x and above.


------------------------------------ LECTURE 20 -------------------------------------

Spring Security is a powerful framework that is used to handle authentication & authorization. ( who to allow and whom to not, right credentials ) & ( if access, what can you do, admin/read/write access )

A dependency is added in pom.xml to secure all the end-points, it uses HTTP Basic Authentication. (encoded-string)
The client sends an authorization header --> Authorization: Basic encoded String, The server decodes thr string, extracts username & password and verifies them.
Access is granted, if they are correct.
username:password  --> it is encoded in base64
Spring Security will generate a default user with a random password that is printed in console logs.

Run you Spring app & a passowrd will be generated
Goto postman, hit any GET journalentry and then hit send, it will say 401 Unauthorized, goto auth -> basic auth, type username and the password from springboot and hit send.
( Remember that this username & password is different from the users we created )
Now it will show authorized and an OK Status with your entry in response.

Now we need Customize Authentication because it was only done for a single user.
@EnableWeb@Security --> Web Security support is enabled. It's used in conjunction with @Configuration
WebSecurityConfigureAdapter is a utility class that provides default configurations & allows customization of certain features. By extending it, you can configure &
customize your spring secity for your application needs.

http.authorizeRequests --> starts authorizing the requests
.anyMatchers("/hello").permitAll --> HTTP request matching the part hello should be allowed for all users whether auth or not
.anyRequest().authenticated() --> Specifies any request, not matched by previous matchers should be authenticated, meaning users have to provide valid credentialsto access the endpoints.
.and() --> method to join several configurations.
.formLogin() --> Enables form-based authentication. It will provide a form for the user to enter their username and password. If user is not authenticated and try to access a secure endpoint, they'll be 
redirected to the default login form.
Spring security provides an in-built controller that handles /login path. It is responsible for rendering the default login form when a GET request is made to /login.
Spring also provides logout functionality. .logout(), post request to /logout will log the user out & invalidate their session.

Basic Authentication is by its design stateless. ( 2nd request doesn't know, what was 1st request ) , All 100 requests are made individually.

Spring Security:
Session Creation
Session Cookie
Security Context
Session Timeout
Logout
Remember Me

We want our SpringBoot application to authenticate users based on their credentials stored in a MongoDB database.
Users and passwords(hashed) will be stored in mongodb & when users try to authenticate, system checks for provided creds and match against what's in the db.

Now we have to create a UserDetailsServiceImpl which is other than our business logic and it is an implementation of an interface with methods already defined.

Make changes, as the Public Controller now has the ability to add user, and UserController deletes and updates user.
All authentication done, some testing through postmn required!!



------------------------------------ LECTURE 21 -------------------------------------


We will be working on Journal Entry Controller and will add authentication to it and then there's no need for @PathVariable as we are sending userName and password
through auth and can be checked in Postman.
Goto Postman and localhost:8080/journal and select Auth and then BasicAuth, provide the username and password.
It was done for GetMapping, now do same for PostMapping, add those 2 lines of authentication and remove any pathvariable.

Postman was giving unauthorized error, it can be solved by right providing the username and password in auth and make sure its encrypted and not plain text format
and also for this project there's no need to use @Transactional, it took couple of hours for me to know where the actual problem lies.
Either remove it or use the Mongodb Factory for it.
Now its working very fine!!!! :)
Check the mongodb atlas collections and there will be users and journal_entries under the journaldb collection with the entity having same id.

CreateUser, then Add journal entry and then get by id.... Make sure to use the basic auth & provide the raw username and password.
Delete the journal entry as well by providing the id from Mongodb Atlas and it should be removed overall.

ALL HTTP VERBS ARE WORKING AND ALL DONE WORK ON GET,PUT,POST,DELETE & BY THEIR RESPECTIVE IDS.
Checked on MongoDB Atlas as well. Authorization done by raw format and object id taken from mongodb for id.


------------------------------------ LECTURE 22 -------------------------------------

Role Based Authorization is done here. A new API and controller is created for admin so only admin can see all users and entries while common users can't have access
and wouldn't be able to see the changes and all users and can't use someone's auth.

We will create an a new admin controller which will fetch all the details, but before that we have to make a change(already done) in spring security by adding role of
admin as has Role for a specific role.
So to allow a specific person to be able to see or make changes, goto mongodb atlas and the user you want to make an admin and edit its user entry and type ADMIN in it.
By updating and editing the role of a specific user, enter its basic auth in Postman and boom!! You can see all the entries and at the same time if you try to enter
anyone else's basic auth, it will say 401 Unauthorized.

We have to create first user and admin manually so we can create or change after and even make a method to create new admin in the controller and UserService as well
by giving it role of a USER as well as an ADMIN as given in the SpringSecurity class. 
So basically this manually created admin can further create an admin and that admin will also be able to see users and entries by enabling its username and password
as auth just like the first admin.

Basically by creating a new user first and then, first manual admin, you will create a new admin from its basic auth, then the created admin will use its raw username
and password in get all to see all
the entries and users

------------------------------------ LECTURE 23 -------------------------------------

In this lecture we will see if there's any other way to write or handle application.properties as we had been writing our setting in it, so we can try to pass
command line arguments in spring boot application.     ----> Basically a YAML lecture... !

Classpath has jars, class and configuration files.
SpringBoot finds itself the class path, src --> main --> resources --> application.properties
Dependenceis are also packaged in classpath.

So another way to write configs is YAML.
It doesn't need to have tags rather it is human readable and has indentation.
YAML aint't markup language !! :)
.yml is extension.

Copy paste that same app.properties but use .yml. Now use this sort of indentation and we don't need to over write things and again and again.

spring:
  data:
    mongodb:
      uri: mongodb+srv://sobi7:test1234@cluster0.ar8adhd.mongodb.net/?retryWrites=true&w=majority&appName=Cluster0
      database: journaldb
      auto-index-creation: true

You have to write a new property from starting and just use : and hit enter to move to next line. Intellij is smart and can recognize.
You can keep poth .properties and .yml but priority is application.properties.

You can provide context path and port of your choice as well but remember that context path then has to be added before every api here in this case of project, /journal
and port can be changed from 8080 to 8081 and would work fine.
application.properties can be deleted now and project would work.

Now almost same can be done in terminal, just write mvn package and then in test you can see jar and can run it by java -jar <name of jar> --property=value
So you can use properties in terminal as well.
So priority is:
application.properties
command line terminal
application.yml


------------------------------------ LECTURE 24 -------------------------------------

JUnit TESTING IN THIS LECTURE!!
API has to be tested. Unit & Component Testing.
Java Unit is a framework for testing -->Java Unit
All tests have to be written in tests, maven ignores test folder so it is not included in jar.

You can write tests as methods and class name can be same as the ones from main.
Make a package called service for example in test and add class UserServiceTests.

@Test ---> This annotation is used on methods that we need to test
assertEquals()  --> this contains parameters for expected and actual value, you can run and check if this is true. This will be imported as static import.

assert ---> there are couple of option after you write assert and you can choose what to test in what context.
You can also add a message after providing parameters in test by comma separated so it tells, which test actually failed.

Now we try to test findByUsername from UserService and we @Autowire userRepo, make a method and use assertNotNull just to know that answer should not be null.
Something like this: userRepo.findByUserName("username here that has been created.");

Make sure to add
@SpringBootTest --> this runs tests like actual project as there is no springboot app context in tests therefore without it, it will fail to run.

@ParametrizedTest --> This annotation is used when you want to carry multiple tests on a single method with changing values and don't want to write method again & again.
Values are fetched here and how they are fetched? They are fetched through another annotation where you can give files:
@CsvSource({
})
Each line in csv represents one test for the parameters.
"1,2,2",
"2,10,12,
"3,3,9"

@ParameterizedTest
    @CsvSource({
            "1,1,2",
            "3,3,9",
            "4,5,13"
    })
    public void test(int a, int b, int expected){
        assertEquals(expected, a+b);
    }

Now if we run a class and want to disable a test we have annotation for that too:
@Disabled --> it disables the method that you don't want to test.

@EnumSource --> another same as CsvSource
@ValueSource ---> this annotation is almost same as CsvSource but as we are not giving any file, we can add ints, or strings to it follwed by = sign and then whatever
you like to test.
@ValueSource(strings = {
"Taha",
"Disha",
"Sobi"
})

@ArgumentSource --> This is long and a little difficult to understand and part of JUnit testing & not exactly part of a SpringBoot therefore can be ignored & above
provided tests can be implemented and are very effective.
But it is basically used for custom arguments by making a new class of that implements ArgumentsProvider & a method @Override of Stream that extends ?Arguments
and builder for testing the entity variables.

Code coverage / tests ---> This is used to show or tell that how much of your project or lines of code have been tested so a plugin is installed called
Code Coverage for Java. ( Not exactly available in this IntelliJ version )
BTW it shows coverage in % when run on tests. It creates a report.
You can export report as well.

@BeforeEach ---> It will run before each test present. If we have to initialze something before each test, then this annotation has to be used.
@BeforeAll ---> This method test will run before all & any.
@AfterAll ---> This if used on a method will run after all the tests.


------------------------------------ LECTURE 25 -------------------------------------

@MockBean ---> this annotation is used to mock, actual service / dependency is mocked.
Now eveytime entire application takes time, all things are injected and autowired, connection to database is made so it takes a lot of time, so to make things faster,
we have to use mockito.
Dummy things are made so a mock db can be made and the actual connection to real db is not made and no time is wasted.
Before findind any specific username, we use the --> when, thenReturn

@InjectMocks ---> Injects the mocked dependency
We are doing all this because @Autowired was used, @SpringBootTest was used so they were loading the entire spring application for test purpose but here we want to mock
things and not use the actual ones, therfore we have to remove @SpringBootTest and @Autowired so @Mock ( @MockBean is used with spring context only ) & @InjectMocks are
2 annotations used for only working with mocks and not loading the spring boot application context whiich takes time to load. By doing so, we are preventing our tests
classes and methods from being components.

The application would still fail so we have to write a method of @BeforeEach, in which we initialize Mocks and inject them too.
Now the initialization is done and our mock/fake test will run smooth and no actual connection will be made.



------------------------------------ LECTURE 26 -------------------------------------

Profiles --> Dev & Prod Environment
Dev env is usually testing, fake users whereas prod env is real with real users.

Difference comes in configurations when in Production Environment, SpringBoot gives us flexibilty of changing configurations based on Environment.
By default we have application.config / application.yml, but we are creating 2 more here : application-dev.yml & application-prod.yml

This is how you will write you application.yml first:
spring:
  profiles:
    active: dev ( write name of profile you want to run, either dev or prod )

You are basically running the application-dev.yml ---> which was actually copy pasted from application.yml and it was replaced with above code and it runs fine!!

If there are exact same config in both of the profiles, you can write them in application.yml, rest of the changes are done in separate profiles.
Rather than rewriting and changes dev / prod again and again, you can goto environment variables in IntelliJ and write spring.profiles.active=dev

Now in production, you aren't going to use IntelliJ, you will run jar.

You can do all the above things in the command line using maven and jar.
.\mvnw clean package -D spring.profiles.active=dev   ---> jvm system properties
&
then moving to our target folder where jar is created, in command line:
java -jar <name of jar > --spring.profiles.active=dev  ---> variables pass

Jenkins do all the things automatically, no need to do all these things and it does everything by just a single click.
We can whitelist the server that load everything from them so we set active profile of prod or dev as you like because server cannot be accessed by local machine, things
work entirely different on server, therefore set profile either on environment variables or use Jenkins.

Bean made based on profile.
@Profile("dev")
You can add annotation to a class so that it tells project if it belongs to dev or prod
 
You can do the same for tests classes as well by adding an annotation:
@ActiveProfiles("prod")

You can get the environment context as well by initializing variables of spring application and using it with getEnvironment(), so on starting the project it shows 
which profile is active or not.


------------------------------------ LECTURE 27 -------------------------------------

This lecture is regarding Logging.

Logging is an essential aspect of application development that allows developerss to monitor & trouleshoot their applications.
Unexpected behavior on production of API if crashed, we can check by LOGGING, either logs are printing or not.

Logback, Log4j2 & Java Util Logging(JUL) ---> 3 frameworks are available in springboot.
Logback: logging framework that offers flexible config & good performance & a default in spring boot apps.
Log4j2: Async logging & support for various output formats.
JUL: default logging standard in java standard edition. --> it's part of JDK.

Usually we use logback. Simplicity & flexibilty in spring boot.

To customize logging, you can create logback.xml file in src/main/resources.

Logging levels: Help in categorizing log statements based on their severity.
Trace, Debug, Info, Warn, Error

Annotations used:
@Slf4j ---> this will be used in springboot app & our project
@Log4j2

First initialize the logger instance in a class where you want to implement Log. We are doing in JournalEntryService. Like this:
private static final Logger logger = LoggerFactory.getLogger(JournalEntryService.class);
<use class where it is implemented>
logger.error("Error occured for {} :",user.getUserName(),e); ---> this is how it is implemented when we have to initalize entire line above, but we can simply use
annotation as well.
@slf4j ---> when this annotation is used, there's no need to initialize that entire line of private static final. Just in catch bloack, use (log.), and it will work.

simple logging facade for java.
slf4j is logging abstraction framework. 

Customization has to be done to make Trace and Debug run, the rest can run like that.

This is how customization is done in application.xml for DEBUG and TRACE.    
logging:
  level:
    net:
      engineeringdigest:
        journalApp: DEBUG

But it is better to use .xml and not .yml because xml is basic for logging therefore that's what we are going to do.
Appender means where do you want to print your logs. Console & File appender.
Create a new file in resources called logback.xml and write the contents like this:
<configuration>

    <appender name = "myConsoleAppender" class="ch.qos.logback.core.ConsoleAppender">
        <encoder>
            <pattern>
                %d{HH:mm:ss.SS} [%thread] %-5level %logger{36} - %msg%n
            </pattern>
        </encoder>
    </appender>

    <appender name = "myFileAppender" class="ch.qos.logback.core.FileAppender">
        <encoder>
            <pattern>
                %d{HH:mm:ss.SS} [%thread] %-5level %logger{36} - %msg%n
            </pattern>
        </encoder>
        <file>
            journalApp.log
        </file>
    </appender>

    <root level="INFO">
        <appender-ref ref="myConsoleAppender" />
        <appender-ref ref="myFileAppender" />
    </root>

</configuration>

So info in this is customizable and modifiable, therefore no need to remember and learn it entirely.
It's creating logs in File as well as on console as well.
We have to give path to <file> outside of project. So that logs are pasted there in root directory in Windows and Linux.
There's RollingFileAppender class and it has RollingPolicy which further contains fileNamePattern, maxFileSize & maxHistory.
As it is seen that this .xml for logging is heavily customizable.


------------------------------------ LECTURE 28 -------------------------------------

Code Quality Check ---> SonarQube, SonarServer, SonarLint
It can be installed either locally on a system or on its own SonarCloud.
Code also has to be present on GitHub.

Download SonarQube & extract it.
Go to its folder, bin & sonarcube, hit and run. It will open an interface and username,password is admin.
Goto your pom.xml and add the dependency for the same SonarQube you just downloaded.
Goto your project terminal in IntelliJ and do mvn clean install sonar:sonar   ( make sure to disable the tests )
Now goto your sonar app and you can refresh and see that our project is loaded and quality is checked. Sonar has rules in it that checks for bugs, vulnerabilities,
coverage & tests and provide you overall quality of your project while giving you recommendations for improving it.

You can also download SonarLint (a plugin in IntelliJ) --> SonarLint is not available now on IntelliJ, SonarQube is available, so we are going for it.
Same rules in SonarLint are also available in SonarQube.

SonarCloud --> signin to sonarcloud and it will redirect you to page where you can create manual setup for GitHub and make your project available on server through
github. After all the config on sonarcloud.io  you will see the interface but nothing from your project, then from Choose your analysis method, choose GitHub Actions.
It will lead through couple of steps and their description and details are also provided about where to do and how. Enter dependency in your pom.xml & then create a 
yml like this .github/workflows/build.yml ( code already provided there on sonarcloud.io)
Open your folder location, open Git Bash and you can write:
git init
Open .gitignore and add .yml & .log under ### STS ### & also add a folder /htmlReport under ### IntelliJ IDEA ###
git status
git add.
git commit -m'first commit"
git config --global user.email "that fake sonarcloud github email"
git config --global user.name "that fake sonarcloud github name"
git status
create a new github repo & search for ..create a nwe repo on command line
git remote add origin <that entire line from github page>
git push origin master ( signin with browser)

You can create that workflow and yml in command line as well.
And there are some steps that you need to follow, so that you can completely see your project on SonarCloud through GitHub.

SonarQube & SonarCloud are doing the same thing, but the difference here is only that Qube is directly integrated in IntelliJ and can be downloaded for integration
whereas Cloud is available on server and to access it, you have to go through github actions.


------------------------------------ LECTURE 29 -------------------------------------

We will see External API Integration in SpringBoot in this video.

Hit external API in code & get the response, everything that is done from Postman must be done from code for external API.
For this lecture we will consume 2 APIs, one is weather & other is quotes.

Well the platform ( theysaidso.com ) --> the one for quotes API, they are not providing free APIs now, but in video, they were free.
Anyways if free, we get they key & register ourselves.
An API token is generated, copy it.
Now use another weatherstack.com ( it's free and generate the token and copy it )

36cf53c9eeaed245d9798a0a38dd0e0b ---> API Key (weather)
We will use api in springboot application.
Just made a @GetMapping in UserController and now make a new request in the heading of user in postman. 
Well you can check if your API is working on a browser but make sure that it is http because this API doesn't work on HTTPS. It looks something like this:
http://api.weatherstack.com/current?access_key=36cf53c9eeaed245d9798a0a38dd0e0b&query=NewYork
The documentation is already available on their website. 
For that we will create a service called WeatherService and do our api work in it.

Make a JSON to POJO online.
Create a package api.response & create a class within it called WeatherResponse and paste the pojo in it.
As you create it will be in snake format, you have to cover it in Camel format, to do that, you will write annotation above each attribute that needs to be changed:
@JsonProperty("observation_time") --> the one seen on http website from json to pojo  ( it's basically a good practice, it would work anyway)
while what we want is this entity: observationTime... ---> So we have to tell our program what to do about it.
Now pass the pojo class in api class created above.

Add an instance of RestTemplate in journalApplication & add @Bean annotation.

Well it worked very fine on first try... ---> The problem is that Edge browser never lets run http, so you never know the response from api until unless you complete
your work and hit it through postman.


------------------------------------ LECTURE 30 -------------------------------------

PostMapping in this lecture of that same API through Postman but in our WeatherService class using only httpEntity.
By a simple change and call, you can send the post call.

HttpEntity<> httpEntity = new HttpEntity<>();
ResponseEntity<WeatherResponse> response = restTemplate.exchange(finalAPI, HttpMethod.POST, httpEntity, WeatherResponse.class);

You can also pass headers from your code as well that are shown in postman like this:
HttpHeaders httpHeaders = new HttpHeaders();
httpHeaders.set("key","value");


------------------------------------ LECTURE 31 -------------------------------------

APIs have become so important to learn because they help a lot in automation.
PipeDream ---> tool to automate
IIElevenLabs ---> Documentation for API, how to integrate, how to work with it.

Goto elevenlabs.io/docs/api-reference/text-to-speech
They have uploaded a jar to their server, they have exposed an endpoint so we can use.

We have the parameters like: Header,Path,Query & Body...
Their entire documentation is automated through api and everything is available and shown & ready to use with the http vers like GET,POST,PUT & DELETE.

sk_0880246c3c47dcfb02cc00e82334585675ac48b7f917005d ---> its my api key & I've to use it on the platform.
I've put the api key so it can be used as required when doing things with your API, I've generated text to speech & its awesome.
We are doing doing all this using UI, but on the side a cURL is given, we can do things with it. Copy it and goto postman and import a new cURL. And you can see the 
Headers & Body. On hitting send, You can get the response below...!!! And cool thing is that you can download it too...
This API can be reused in code and only you can change text later on or view documentation for further cool options. 


------------------------------------ LECTURE 32 -------------------------------------

Till now we've been using these 2 annotations to create a bean:
@Component
@Bean

Problem is coded in service. Actual code.
You can use this annotation, for readibilty, to tell someone in future that this class is service and business logic is written in it. Any of them can be used now.
@Service


------------------------------------ LECTURE 33 -------------------------------------

Our hard coded api key is not a good practice as it will be public in uploaded on server of github.
We will write api key in yml --> application.yml & use it in our code.

@Value annotation is used here. As we are entering our api key in application.yml therefore we will pass the property in paranthesis like this:
@Value("${weather.api.key}")

Not working because have to remove final as well as static.
Static variable is related to class, not the instance.

Your hard coded api has static & final keywords but when it is done in your application.yml, it has to be done through @Value & have to remove the static and final
keywords for initializing that api key.

------------------------------------ LECTURE 34 -------------------------------------

@PostConstruct
When implemented on a method, as soon as bean is created, immediately that method is invoked.

If want to save api in a Database, we will make it save in a collection.
We are going to make another collection for the sake of saving api, it means we are going to configure api in database as it is recently being used and we don't want it
to be used through our code.

Latency is increased, means one hop is increased when called from server and then it is hit in our code, so we will see another practice called application cache.
It is used in frequently used & frequently changing config to database & then load that database inside your springboot application.

We will create a package called cache and a class inside called AppCache and make it a bean by using @Component.
Write a method and before that initialize a String Map.
Now create a collection in MongoDbAtlas called config_journal_app. Create insert document and write the key value pair, in which the key is name and value contains
the api value. The parameters have to be in the angular brackets like <apiKey> & <city>.

Make a repository & pojo of our collection.
Autowire the configjournalapprepo in AppCache class.
Now what we are doing is that we are making it static not by keyword because it is now not coming from our MongoDB Atlas but loaded like a cache to a variable in 
our springboot app just once.
Yess... It's working perfectly fine!!

More work could be done, just for good coding practices like creating enum, placeholders class but not necessary for your project...
You can use clear cache so even if one time your api is loaded, it can be cleared using postman and in big projects if we do some changes in database we don't want our
application to crash for say if we chaange our API, it still has to work because it is once loaded and cache could be cleared now and there will be no response from above
written code but all response from lines of code below..


------------------------------------ LECTURE 35 -------------------------------------
 
MongoTemplate & Criteria.
Till now we were interacting with our mongo through MongoRepository which contains the implementation & interfaces, which are generally autowired where they are used,
but we have to be very careful about it because we are doing here Query Method & if name or param goes wrong, you won't be able to interact with your database and will
need to check the documentation for it, so now we will learn Criteria.
Create a new class in package repository called UserRepositoryImpl. Criteria & query goes hand in hand.

A new annotaion is introduced here:
@MongoTemplate --> This is class that is provided by spring data mongodb and automatically everything is done behind the scene. This provides abstraction, configured by
springboot, no need to go to its implementation.
Create 2 variables, email and boolean sentimentAnalysis in entity User class.
In this UserRepoImpl, make a new variable & initialize in a method called Query and use it with .add to add things.
We are using criteria which is making things a lot easier rather than just interface of Repo where there are chances of error.
To interact with this new criteria, use mongoTemplate as autowired before with .find and query as param and class where you want to send.

We are basically working here for sentiment ananlysis like by the end of week, a mail will be received by the user that how his mood was this entire week, for which
we will use NLP later on.
In this Criteria can be used for OR, AND by making an object of Criteria and using .orOperation / .andOperator with the statement being comma separated in them.
By writing all this, we could reduce it by adding regular expression. So basically users must have valid email & their sentimentAnalysis must be true too.

Now to go mongodb atlas and take 2 users in which, one of them contains right email format and other one with wrong and SA being true.
Goto code and debug tests and methods for SA, run and check if users are available..
YESS ! Valid users are returned by method.

You can make conditions where you can use .nin and in for not-included & included for allowing or not allowing any entity variable.


@Autowired
    private MongoTemplate mongoTemplate;

    public List<User> getUsersForSA(){
        Query query = new Query();
        //query.addCriteria(Criteria.where("userName").is("Hassy"));
//        query.addCriteria(Criteria.where("email").exists(true));
//        query.addCriteria(Criteria.where("email").ne("").ne(null));
        query.addCriteria(Criteria.where("email").regex("^[A-Za-z0-9._%+-]+@[A-Za-z0-9.-]+\\.[A-Z|a-z{2,6}$]"));
        query.addCriteria(Criteria.where("sentimentAnalysis").is(true));
        List<User> users = mongoTemplate.find(query, User.class);
        return users;


------------------------------------ LECTURE 36 -------------------------------------

Sending mail using code through IntelliJ.
Gmail, SMTP is covered in this lecture.

Add dependency of mail in pom.xml
Create an EmailService class in service and Testclass as well in tests.
Now we will be able to send email through JavaMailsender by Autowiring it, add email functionality in springboot application.
You will also have to write application.yml like this:
spring:
  mail:
    host: smtp.gmail.com
    port: 587
    username: email@gmail.com
    password: xxxx xxxx xxxx xxxx
    properties:
      mail:
        smtp:
          auth: true
          starttls:
            enable: true

Secure transaction is done here but the thing to remember is that email should be valid and you don't put your actual password here, no hardcoding.
Rather you will have to create a passkey / app / security key for this method as it provides functionality for some older version, but it's not working here because
Google doesn't generate it now, and systems have become modern so security is very high yet passkeys cannot be generated directly through google and MicrosoftEdge won't 
allow you to do through it.

 @Autowired
    private JavaMailSender javaMailSender;
    
    public void sendEmail(String to, String subject, String body){
        try{
            SimpleMailMessage mail = new SimpleMailMessage();
            mail.setTo(to);
            mail.setSubject(subject);
            mail.setText(body);
            javaMailSender.send(mail);
        }catch (Exception e){
            log.error("Exception while sendEmail:", e);
        }
    }

It's very easy as you can use the instance of mail. and it has variety of options to include.

Well it was difficult, yet managed to setup the passkey by adding pin and password to laptop and other google services so passkey can be generated.
In your google account settings --> Security --> App password ( You may find it difficult to get to it because at first it's not present there )
Generate a new name and password, paste the password in your application.yml and send the mail.

You can use 465 as 587 could usually be blocked by ISP or office or even your local machines.
Make some changes to app.yml so it will work now:

          ssl:
            enable: true
          socketFactory:
            port: 465
            class: javax.net.ssl.SSLSocketFactory
            fallback: false

It took some time to debug and troubleshoot but finally I've received mail from my test class.

------------------------------------ LECTURE 37 -------------------------------------

If you want to automate things & they are done on regular interval, so using Crons & Schedulers you can achieve this automation.
Now we will send our users sentiment analysis every sunday morning 9am, now it's not done through manual but automated work. --- scheduled task
We will create a service called SentimentAnalysis & create a String method in it. Create a package called scheduler and make a class called UserScheduler in which the 
method fetches the users & send email automatically.
Autowire the SentimentAnalysisService, EmailService & UserRepoImpl

For sending email automatically, we have to add an annotation to this method:
@Scheduled(cron = "")   ---> it does the work for us automatically.
By using cron in parameters, we tell that when this method has to run. Therefore you need to pass a cron expression. Like this:
0 0 9 * * SUN   ---> 9am Every Sunday ( You can generate it through cron maker website )

To run this, you also need to tell your SpringBootApplication that we are using some scheduling in our project. To do that, we add annotaion in main app:
@EnableScheduling

We are doing another thing that we are going to add appCache to this scheduler so it refreshes every 10 minutes through cron and if an API changes or something happens,
we won't need to restart our springboot app because it is stored in local cache.
(cron = "0 0/10 * ? * *")   --> this is cron for appCache.


------------------------------------ LECTURE 38 -------------------------------------

Create a package enums & create an enum in it containing HAPPY, SAD, ANGRY, ANXIOUS.

We are going to be making a lot of changes in our code now... The github link will be provided of updated code, but I'll be editing most of it by self.
We are deleting the SentimentAnalysisService and making changes in our UserScheduler class directly.

Write test case as well in test package.
Most of changes are already done and they were all in UserSceduler. What had to be done was as we had added a Sentiment enum, we will add to our journalEntry Entity too.
Then we will map that Sentiment rather than using text or String to get the sentiments, count them, compare them and send email as per our cron job.



------------------------------------ LECTURE 39 -------------------------------------

Redis is an open-source in-memory data store as cache, vector database, document database, streaming engine & message broker.
Basically the need to use Redis is that for example, if there's a website that only takes user's ip and location that where the user is viewing the page from, will the 
server load entire different new reload for every client that visits that site.? NO....
It will store in RAM because it's faster and does its job in nano seconds than hard disk that is comparatively slower than RAM.
We are not going to load entire pages and render the front-end and hit the backend for every user that visits. It will have to goto code and that http verb and then find
the database, we don't have that much time.

So to get everything, we will implement Radis in our code, so response is faster, and the backend when hits, first checks the radis, if information asked is available 
there or not. We can't be using appCache for everything.
The computation in first time using Radis will obviously take time but when a response is once generated, it goes through Redis and stores it, and it has a TTL field
which helps in making an actual connection every few hours.

Install radis. Now the thing is that radis is not officially made for Windows therefore first you have to install wsl.
Goto powershell:
wsl --install
It will install Windows Subsystem For Linux.
Now reboot your system so effect can be seen.
Search WSL in your search bar and it will open wsl. Go for basic setup
Set your username and password and you're good to go.
Now check the documentation of redis for windows, do exactly as said for installing radis through your ubuntu.
This are the commands you need to follow to run redis:

curl -fsSL https://packages.redis.io/gpg | sudo gpg --dearmor -o /usr/share/keyrings/redis-archive-keyring.gpg
echo "deb [signed-by=/usr/share/keyrings/redis-archive-keyring.gpg] https://packages.redis.io/deb $(lsb_release -cs) main" | sudo tee /etc/apt/sources.list.d/redis.list
sudo apt-get update
sudo apt-get install redis
sudo service redis-server start
redis-cli

Test the connection by sending ping & reply must be PONG.

Add dependency of Radis in pom.xml.
We have to write radis properties in application.yml
spring:
  redis:
    host: localhost
    port: 6379

RedisTemplate is use to interact with Redis. We are creating a test class to test Redis by autowiring RedisTemplate. A method is written to set and get the email.
Serialization & Deserialization are not in sync now. Springboot & redis cli are currently not able to get or set anything because their serializers & deserializers
arenot working together, to make them work they must be same.
In SpringBoot we have to manually set serializer & deserializer.
Therefore we will make a RedisConfig class in config package.
Any method annotated with @Bean in a class, that class has to be annotated with @Configuration.
We will write a method & RedisConnectionFactory is given as param in method whereas the method itself is a RedisTemplate. We set Key & Value Serializer.

Response will be saved on first api hit to redis. It saves time and cost.
We will create a RedisService and response is saved to Object. We can write POJO of Object as well. We will make a get & set method & also implement this in 
WeatherService to hit that api in Redis-cli.
By this: weather_of_Karachi ---> it gives all the info and its correct!!! :) 
We are able to see JSON because we are setting key with JSON and it will get all the info.

We are getting response on Redis-Cli by setting in our RedisService a& WeatherService from IntelliJ.
We can set the key and get the reponse through debugging in our WeatherService following RedisService.
Our API call doesn't hit now and the response is quick and returning POJO. it's effective and that's why it is used, it didn't even have to reach the API, everything
was got by the RedisService and it displayed.

RadisCloud can be used just like MongoDb Atlas and can be set in application.yml.


------------------------------------ LECTURE 40 -------------------------------------

Integrate Radis Cloud with Springboot Application.
DIY   :)


------------------------------------ LECTURE 41 -------------------------------------

We will cover Kafka in this lecture....
Open source distributed event streaming platform ---> Can run on multiple serves(machines are connected).
KAFKA is designed to handle data that is constantly being generated and needs to be processed as it comes in, without delays.
It ensures flow of data from source to destination smoothly.
For Example Social media platform where people are adding posts, liking, commenting. It's realtime and needs to be seen by all constantly. Kafka comes into play.
Saves API calls. 
It has asynchronousity. Scalability is achieved here as the data is distributed across multiple servers running parallel to handle millions of requests.

Kafka clusters: group of kafka brokers ( server where kafka is running i.e multiple servers running kafka)
Kafka producer: writes new data in kafka cluster.
Kafka consumer: retrieves data from kakfa cluster.
Zoo keper: keeps track of kafka cluster health.
Kafka connect: source & sink ( data can be retrieved without writing any code i.e declarative integration. From source to cluster & vice versa to sink )
Kafka stream: used to transform data.

Goto this site:
https://kafka.apache.org/downloads  --> And download the latest kafka .bin file. Extract the zip file to any folder.

You can open command line in your kafka folder and goto 
cd Windows and can run any file you want. We will run zookeeper so command is like this:
zookeeper-server-start.bat ..\..\config\zookeeper.properties --> and it runs on port 2181

Another command to be run on another terminal:
kafka-server-start.bat ..\..\config\server.properties  ---> it will start kafka broker

Open 3 more cmd and run the command, one for topic creation, one for writing into topic--> data produce & one for getting from topic --> data consume:
kafka-topics.bat --create --topic my-topic --bootstrap-server localhost:9092 --replication-factor 1 --partitions 3
kafka-console-producer.bat --broker-list localhost:9092 --topic my-topic
kafka-console-consumer.bat --bootstrap-server localhost:9092 --topic my-topic --from-beginning

Topic is just like a table where we store similar data.
Now anything you write in the topic will instantly be displayed in consumer cmd.

KAFKA TOPIC:
Named container for similar events. Unique identifier of a topic is its name.
Student opic will have related data. Food topic will have related data.
They are like tables in a database.
They live inside a broker.
Producer produce a message into topic ( ultimately to partitions in round robin fashion ) or directly to partitions. Consumer poll continuously for new messages using 
the topic name.

Partition: A topic is partitioned and distributed to kafka brokers in round robin fashion to achieve distributed system.
Replication Factor: A partition is replicated by this factor and it is replicated in another broker to prevent fault tolerance.

Producer ----> Topic ----> Partition

PARTITIONS:
A topic is split into several parts and known as partitions of the topic.
Partition is where actually the message is located inside the topic.
Therefore while creating the topic, we need to specify the number of partitions ( it is arbitrary and can be changed later)
Each partition is an ordered, immutable sequence of records.
Each partition is independent of each other.
Each message gets stored into partitions with an incremental id knows as its offset value.
Ordering is there only at partiton level
Partition continuously grows.( offset increases) as new records are produced.
All the records exist in distribuuted file log.

If message is sent without key, data will be saved in partitons as round robin so consumer doesn't get message in sequence.
As ordering is done at partition level, so in sequence it can be stored in only 1 partition with help of partitioner which checks for key and applies hashing with data.
Same key --> Same partition
Two things ----> key & value, key is optional
We can send message with or without key.

When sending data with key, order will be maintained as they will be in the same partition.
Without key we cannot guarantee the ordering of messages as consumer poll the messages from all the partitions at the same time.

kafka-console-producer.bat --broker-list localhost:9092 --topic my-topic --property "key.separator=-" --property "parse.key=true"
kafka-console-consumer.bat --bootstrap-server localhost:9092 --topic my-topic --from-beginning -property "key.separator=-" --property "print.key=false"

hello - apple
hello - mango
hello - kiwi
These are with keys: hello is key, and rest is message.
bye- guava
bye - banana

All these will appear in consumer as ordered because they are stored in different partitions in Topic due to their different keys.
And they are going to be in different partitions as their group ids will be different as if more consumers are created in a group, they will hold an offset as well.

CONSUMER OFFSET & CONSUMER GROUPS:
Position of a consumer in a specific partition of a topic. It represents the latest message, consumer has read.
When a consume group reads a messgae from topic, each member of the group maintains its own offset and updates it as it consumes messages.
__consumer_offset --> built-in topic that keeps track of latest offset committed for each position of each consume rgroup.
The topic is internal to Kafka cluster & not meant to be read or written to directly by clients. Instead, the offset information is stored in the topic and updated by
 the Kafka broker to reflect the position of each consumer in each partition.
The information consumer offset is used by Kafka to maintain the reliability of the consumer groups and to ensure that messages are not lost or duplicated.

No of consumer group = No of consumer offset
The __consumer_offset topic is used to store the current offset to each consumer in each partition for a given consumer group. Each consumer group updates its own offset
for the partitions it is asigned in the consumer offset topic.

When a consumer join a consumer group, it sends a join request to the group co-ordinator. The group co-ordinator determines which partitions the consumer should be
assigned based on the number of consumers in the group and the current assignment of partitions to consumers. Group co-ordinator then sends a new assignment of partitions
to the consumer, which includes the set of partitions that the consumer is responsible for consuming. The consumer starts consuming data from the assigned partitions.
Consumers in a consumer group are always assigned partitions in a sticky-fashion. Same partitions are given to a specific consumer from group until unless it leaves.

SEGMENT COMMIT LOGS & RETENTION:
There's a topic and topic has partitions, each partitions has messages, and one set of message is called a segment. ( we can define segment size )
Messages are stored in commit log actually which is present in our filesystem, in our server.properties, there's a directory of commit log, this is where actual messages
are stored. tmp/kafka_logs --> .log ( commit logs, actual data that kafka producer produces )
Size & Time-based retention policy. 
Log files are encoded, and kafka cleaner is always running in background making sure that if policy meet, it either deletes the oldest data or after some time deletes the
data and new file is also auto-generated when a segment size is reached.

KAFKA BROKERS:
We can create 2 copies of server.properties from config and change their port while kafka server port remains same for all so that they know that they are part of cluster.
name the copies 1 and 2 and also change their broker ids by making them unique. Also change the name of their log files within vs code.
Goto bin and open 2 cmd and start server using your 2 properties files.

Now we will create topic with 3 replication factor & 3 partitions.
Now kafka has 3 brokers( 3 kafka server instances ) & 3 partitions in the topic. Data goes to 1 broker and it is sent to rest of brokers as well for fault tolerance.
In case a broker goes down, consumer can get from other 2 brokers. Replication factor gives fault-tolerance. 
Leader is decided at partition level

Now create a kafka topic with all the servers you just created:
kafka-topics.bat --create --topic my-gadgets --bootstrap-server localhost:9092, localhost:9093, localhost:9094 --replication-factor 3 --partitions 3

Now run a producer and consumer with same all 3 servers and give some values.
kafka-console-producer.bat --broker-list localhost:9092, localhost:9093, localhost:9094 --topic my-gadgets
kafka-console-consumer.bat --bootstrap-server localhost:9092, localhost:9093, localhost:9094 --topic my-gadgets --from-beginning
Now check your C:\tmp/kafka-logs, 1 & 2, data is sent to all 3 brokers.

kafka-topics.bat --describe gadets --bootstrap-server localhost:9092, localhost:9093, localhost:9094     ---> This shows the in-sync replica, leader & replicas.

INTEGRATION WITH SPRING BOOT:
As we don't need to make a mess in our local system, therefore we will use Kafka cloud through Confluent Cloud: Fully Managed Kafka as a service.
Make an account, go through setup similar to other cloud platforms.
As its service is paid, therefore I'm unable to create a cluster, but we will still go through the steps...
cluster_0 is created, click and then goto topics, add a topic name it: weekly_sentiment and default partitions.
Click on your topic name, then configure a client, select springboot, give your topic name. Then below there's a property panel given, you have to exactly paste it in
your application.properties or application.yml

Goto pom.xml and paste this dependency:
<dependency>
	<groupId>org.springframework.kafka</groupId>
	<artifactId>spring-kafka</artifactId>
</dependency>

Now create Kafka cluster API key, it's secret --> both of them have to be placed in that file beside it...
Now we have to write a yml from that file from cluster:
  kafka:
    bootstrap-servers:
      producer:
        key-serializer: org.apache.kafka.common.serialization.StringSerializer
        value-serializer: org.springframework.kafka.support.serializer.JsonSerializer
      consumer:
        group-id: weekly-sentiment-group
        key-deserializer: org.apache.kafka.common.serialization.StringDeserializer
        value-deserializer: org.springframework.kafka.support.serializer.JsonDeserializer
        properties:
          spring:
            json:
              trusted:
                packages: net.engineeringdigest.journapApp.model
      properties:
        security:
          protocol: SASL_SSL
          sasl:
            mechanism: PLAIN
            jaas:
              config: org.apache.kafka.common.security.plain.PlainLoginModule required username='' password'';
          session:
            timeout:
              ms: 45000

Consumer sends signal to broker, if in 45 seconds, consumer doesn't send signal to broker, kafka will send it to another consumer.
What can be done now is that you can Autowire Kafkatemplate in your UserScheduler.java class and use it to send email.

You can create a SentimentConsumerService and autowire EmailService.
Use @KafkaListener(topics="weekly-sentiments", groupId="weekly-sentiment-group")
make a method consume with SentimentData as param and sendEmail(sentimentData) in body
Below that make a sendEmail method with same param as above and use emailService.sendEmail(sentimentData.getEmail(),"Sentiment for week", sentimentData.getSentiment();

Make a SentimentData class with email & sentiment as variables.

Check the weekly-sentiment in messages in your confluent, messages will be there and check your email and it will be there too.
 

------------------------------------ LECTURE 42 -------------------------------------

JWT Authentication & Authorization in this lecture... :)

JSON Web Token is a way to securely transmit information between parties as a JSON object. It is a compact URL-safe token that can carry information between parties.
Base-64 URL-encoding. Its is a string consisting of three parts, separated by dots: Header, Payload, Signature.
Header consist of 2 parts: The type of token(JWT) and the signing algorithm being used(HMAC SHA246 or RSA)
Payload: Contains the claims. Claims are statements about an entity(typically the user) and additional metadata.
Signature: It is used to verify the sender of JWT is who it says it is and to ensure that the message wasn't changed along the way.
To create a signature aprt, you have to take the encoded header, the encoded payload, a secret, the algorithm specified in the header and sign that.

Username & password are only base64 encoded in BasicAuth, therefore they can be decoded everytime they are in the URL.
Sending token and it has an expiry therefore JWT token is preferred.

Add these dependencies in your pom.xml:
<dependency>
	<groupId>io.jsonwebtoken</groupId>
	<artifactId>jjwt-api</artifactId>
	<version>0.12.5</version>
</dependency>
<dependency>
	<groupId>io.jsonwebtoken</groupId>
	<artifactId>jjwt-impl</artifactId>
	<version>0.12.5</version>
	<scope>runtime</scope>
</dependency>
<dependency>
	<groupId>io.jsonwebtoken</groupId>
	<artifactId>jjwt-jackson</artifactId>
	<version>0.12.5</version>
</dependency>

Work in PublicController:
Changed the name of creater-user from PublicController to signup.
Create login in same class by /login
@Autowire the JwtUtil class. @Autowire AuthenticationManager @Autowire UserDetailsService.
Write some logic in login method which is ResponseEntity type.
Now we work on JwtUtil class.

Now from postman, hit the /login by giving a userNmae and password while debug the PublicController login method and creteToken from JwtUtil.
If after debug, you pause the debug and open the postman, you will see a jwt token generated as a response.
Now create a package named filter and make code of JwtFilter.
Make some changes in your SpringSecurity from config package. @Autowire the JwtFilter.
Now when everytime the string(token is generated you can authenticate the user by adding the token to Bearer Token from Authorization in Postman. It has a limit therefore
will expire after sometime. Time limit can be set.
Goto admin and get all users, put the bearer token and hit send ---> you will get all the users and now there's no need for basic auth.. !!!

Filter runs before the control goes to the controller for authentication.

We did login and signup, logout will be from client-side from app.
Token can be removed by client locally.


------------------------------------ LECTURE 43 -------------------------------------


What if Kafka fallsback??

Changing will be done only if you are using the paid version,  because it's not working with free version ,therefore no need to work on this!!
Directly mails can be send direct from personal mail.
It's called Fallback mechanism...


------------------------------------ LECTURE 44 -------------------------------------

Deploying our Springboot application on cloud.
AWS provides you VMs where you can run your apps.
But we will deploy on Heroku. ---> It simplifies things.
Create an account on Heroku and do the MFA through SalesForce Authenticator.
Now add a payment method, just save details   ---->  if not then just follow the steps or lecture if want to deploy later sometime for someone.

Payment of 1$ through Nayapay card.
Now goto main page, scroll down to select java. Now you have to install Heroku cli through git bash. ( Download git if not installed)
Download Heroku installer. After installing heroku, either goto cmd or git and type heroku login & pres any key to move to browser for login.

Now goto your github repo of your project and create a fork. ( If you don't own the project, owner cannot create a fork or if not a part of an organization )
Now open git bash in your project. Type: git push origin main   ( It will say everything up to date if updated project is there)
Now type in git: heroku create ---> It will create app on heroku. It will give a URL where your application will be hosted. Now we can use that link in Postman instead
of using localhost.
Now another URL is given, it is used when code has to be pushed to heroku ( on cloud directly )  --> Remote repo on Git & Heroku
git remote --v  ---> Tell about repo on remote locations

Now type:
git push heroku main

Do this if remote not created because there was no fork:
$ git remote add heroku https://git.heroku.com/safe-stream-41612.git
$ heroku buildpacks:set heroku/java
$ heroku update

Change the values to placeholders in your application.yml
heroku config:set SERVER_PORT=8081
Make sure all variables' values are set in app.yml like this:
key: ${WEATHHER_API}S and all other where we set the values by ourselves.
You can create key,value in your cloud rather than through git or from your intelliJ code.

Now in postman, give our url of heroku app/public/login...
Domain instead of localhost:8080
Now you can't be switching between 2 domains, local or cloud so in postman you can create environments, local and prod in which you can give domain's initial and current
value so that you can run on your cloud and local as well
Now open any get call and select environment and in the call you give the environment like this:
{{domain}}/public/health-check

Heroku captures same error in logs as git console.'
heroku logs --tail

Now when you goto your app dashboard on heroku, goto settings and check logs too...
You can scroll down and see the URL, which means you can give a domain name to it by setting a path in your journalApp by creating another controller.


------------------------------------ LECTURE 45 -------------------------------------

Swagger is used for documentation of APIs.











